{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yKp1NLIIia1"
      },
      "source": [
        "<img src='sharif_logo.png' alt=\"SUT logo\" width=150 height=150 align=left class=\"saturate\" >\n",
        "\n",
        "<br>\n",
        "<font face=\"Times New Roman\">\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=7>\n",
        " Deep Learning <br>\n",
        "<font color=2565AE size=5>\n",
        "Computer Engineering Department - Spring 2025  <br>\n",
        "<font color=3C99D size=5>\n",
        "          Homework 2:  <br>\n",
        "<font color=696880 size=4>\n",
        "           \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRPhazMdFLvc"
      },
      "source": [
        "# Assignment Overview\n",
        "\n",
        "In this assignment, you will explore inference scaling techniques in large language models (LLMs) and evaluate their performance using the Math Benchmark. Throughout the notebook, you will learn about several inference methods, including:\n",
        "\n",
        "- **Chain-of-Thought (CoT):** A method where the model generates intermediate reasoning steps before providing the final answer.\n",
        "- **Best-of-n Sampling:** An approach that generates multiple candidate responses and selects the best one based on a scoring function.\n",
        "- **Beam Search:** A technique that expands several possible sequences simultaneously, choosing the most promising ones based on probability.\n",
        "- **Self-Refinement:** An iterative process where the model revises its output to improve accuracy and coherence.\n",
        "\n",
        "The **Math Benchmark** is a suite of challenging mathematical problems designed to test the reasoning and problem-solving capabilities of LLMs. The benchmark includes a variety of questions ranging from basic arithmetic and algebra to more advanced topics such as geometry and calculus. For example, you might be asked to solve an equation like `2x + 5 = 15` or compute the derivative of a function, tasks that assess the model's ability to handle both straightforward and complex mathematical queries.\n",
        "\n",
        "By the end of this assignment, you will have:\n",
        "- Gained a deeper understanding of inference time scaling methods in LLMs.\n",
        "- Compared the effectiveness of different inference techniques using a rigorous math evaluation framework.\n",
        "\n",
        "Let's dive into the notebook and begin exploring how these methods perform on a challenging set of math problems!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl3XC5vVFLvc"
      },
      "source": [
        "## vLLM: Accelerated Inference Engine for LLMs\n",
        "\n",
        "vLLM is an open-source project designed to optimize the loading and inference of large language models. By leveraging advanced memory management techniques and dynamic batching, vLLM significantly speeds up the inference process, making it easier to deploy and experiment with LLMs even on hardware with limited resources\n",
        "So we use vLLM to get results faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXu7t6KXFLvd"
      },
      "source": [
        "# installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyG3Ng0jIl7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "576dabce-a845-4878-f1ec-cff4a89eee12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vllm\n",
            "  Downloading vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from vllm) (2024.11.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Collecting blake3 (from vllm)\n",
            "  Downloading blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.52.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.32.0->vllm) (0.32.4)\n",
            "Requirement already satisfied: tokenizers>=0.21.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (5.29.5)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.84.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.5)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.22.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.2.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
            "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting llguidance<0.8.0,>=0.7.11 (from vllm)\n",
            "  Downloading llguidance-0.7.29-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Collecting outlines==0.1.11 (from vllm)\n",
            "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lark==1.2.2 (from vllm)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xgrammar==0.1.19 (from vllm)\n",
            "  Downloading xgrammar-0.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.14.0)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\n",
            "Collecting partial-json-parser (from vllm)\n",
            "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting pyzmq>=25.0.0 (from vllm)\n",
            "  Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting msgspec (from vllm)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf>=0.13.0 (from vllm)\n",
            "  Downloading gguf-0.17.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
            "  Downloading mistral_common-1.6.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
            "Collecting compressed-tensors==0.10.1 (from vllm)\n",
            "  Downloading compressed_tensors-0.10.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting depyf==0.18.0 (from vllm)\n",
            "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Collecting watchfiles (from vllm)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting python-json-logger (from vllm)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.15.3)\n",
            "Collecting ninja (from vllm)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting opentelemetry-sdk>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-api>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp>=1.26.0 (from vllm)\n",
            "  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions-ai>=0.4.1 (from vllm)\n",
            "  Downloading opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting numba==0.61.2 (from vllm)\n",
            "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting ray!=2.44.*,>=2.43.0 (from ray[cgraph]!=2.44.*,>=2.43.0->vllm)\n",
            "  Downloading ray-2.47.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting torch==2.7.0 (from vllm)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchaudio==2.7.0 (from vllm)\n",
            "  Downloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchvision==0.22.0 (from vllm)\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.30 (from vllm)\n",
            "  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting astor (from depyf==0.18.0->vllm)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.7)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba==0.61.2->vllm)\n",
            "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting interegular (from outlines==0.1.11->vllm)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Collecting diskcache (from outlines==0.1.11->vllm)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.24.0)\n",
            "Collecting pycountry (from outlines==0.1.11->vllm)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting airportsdata (from outlines==0.1.11->vllm)\n",
            "  Downloading airportsdata-20250523-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
            "  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting sympy>=1.13.3 (from torch==2.7.0->vllm)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->vllm) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->vllm) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->vllm)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0->vllm)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->vllm) (75.2.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.2)\n",
            "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm) (24.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.32.0->huggingface-hub[hf_xet]>=0.32.0->vllm) (1.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.26.0->vllm) (8.7.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm) (1.72.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.26.0->vllm)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->vllm) (0.4.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.2.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.4.26)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.1->vllm) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.20.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading rich_toolkit-0.14.7-py3-none-any.whl.metadata (999 bytes)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.26.0->vllm) (3.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2025.4.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.25.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->vllm) (1.3.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "Downloading vllm-0.9.1-cp38-abi3-manylinux1_x86_64.whl (394.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.6/394.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.10.1-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m818.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgrammar-0.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.17.0-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llguidance-0.7.29-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.6.2-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions_ai-0.4.9-py3-none-any.whl (5.6 kB)\n",
            "Downloading prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading pyzmq-26.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.4/862.4 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.47.0-cp311-cp311-manylinux2014_x86_64.whl (68.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blake3-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.5/385.5 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
            "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading airportsdata-20250523-py3-none-any.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading rich_toolkit-0.14.7-py3-none-any.whl (24 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, blake3, uvloop, triton, sympy, pyzmq, python-json-logger, python-dotenv, pycountry, partial-json-parser, opentelemetry-semantic-conventions-ai, opentelemetry-proto, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, msgspec, llvmlite, llguidance, lark, interegular, httptools, gguf, dnspython, diskcache, astor, airportsdata, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, email-validator, depyf, rich-toolkit, prometheus-fastapi-instrumentator, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, lm-format-enforcer, torch, ray, outlines_core, opentelemetry-sdk, mistral_common, fastapi-cli, xgrammar, xformers, torchvision, torchaudio, outlines, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, compressed-tensors, opentelemetry-exporter-otlp, vllm\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: pyzmq\n",
            "    Found existing installation: pyzmq 24.0.1\n",
            "    Uninstalling pyzmq-24.0.1:\n",
            "      Successfully uninstalled pyzmq-24.0.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed airportsdata-20250523 astor-0.8.1 blake3-1.0.5 compressed-tensors-0.10.1 depyf-0.18.0 diskcache-5.6.3 dnspython-2.7.0 email-validator-2.2.0 fastapi-cli-0.0.7 gguf-0.17.0 httptools-0.6.4 interegular-0.3.3 lark-1.2.2 llguidance-0.7.29 llvmlite-0.44.0 lm-format-enforcer-0.10.11 mistral_common-1.6.2 msgspec-0.19.0 ninja-1.11.1.4 numba-0.61.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-semantic-conventions-ai-0.4.9 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 pycountry-24.6.1 python-dotenv-1.1.0 python-json-logger-3.3.0 pyzmq-26.4.0 ray-2.47.0 rich-toolkit-0.14.7 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0 triton-3.3.0 uvloop-0.21.0 vllm-0.9.1 watchfiles-1.0.5 xformers-0.0.30 xgrammar-0.1.19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "zmq"
                ]
              },
              "id": "48a8fa98482e49979786ca6acde63a22"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWvzLP5FIuc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4f881e-0649-405f-f948-783f2d08a102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp35GisjMQvw",
        "outputId": "7ffb4b9e-c44e-4d14-e1a0-f5e22fd675bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY4FWoryOiry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ef0ba5-6578-4f2c-b005-4317084721f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.6)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 2.3.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivC7LIgkLUvR"
      },
      "source": [
        "\n",
        "This command launches a vLLM inference server with:\n",
        "- Model: `DeepSeek-R1-Distill-Qwen-1.5B`\n",
        "- Port: `8000` (default API endpoint)\n",
        "- Precision: `half` (FP16) for memory efficiency\n",
        "- Max context length: `3192` tokens\n",
        "\n",
        "**Note:**  \n",
        "🔹 Ensure you're using a GPU runtime (T4 or better) in Colab  \n",
        "🔹 Only run the next cell if this one executes successfully\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HNmCEaGIzEe"
      },
      "outputs": [],
      "source": [
        "!vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"   --port 8000   --dtype=half   --max-model-len 3192 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFPitveIL8PT"
      },
      "source": [
        "* this cell lunches model in background using vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZSaM7q5LrC0",
        "outputId": "1e48ef08-1877-48a5-8ed9-601fafe456fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ],
      "source": [
        "!nohup vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" --port 8000 --dtype=half --max-model-len 5192 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0yGjrAiGXWs"
      },
      "source": [
        "## LLM Query Function\n",
        "\n",
        "* This Python function sends prompts to a locally-hosted LLM API and returns the generated response\n",
        "* you can change max_tokens and temperature as you want\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-QntB7EF4mb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def get_llm_response(prompt):\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "\n",
        "        ],\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0.6\n",
        "    }\n",
        "    response = requests.post(url, json=payload)\n",
        "    return response.json()['choices'][0]['message']['content'].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzqLI_OPMNOD"
      },
      "source": [
        "# Test response generation\n",
        "- testing model with some Math benchmark quesions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PG8o90EML-4",
        "outputId": "fe8a3e3f-c25b-4d47-e2db-934f3d527265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alright, so I have this problem where I need to find a way to express a double sum involving reciprocals of cubes in terms of two known sums, p and q. Let me write down what's given:\n",
            "\n",
            "First, p is the sum from k = 1 to infinity of 1/k², so p = 1 + 1/4 + 1/9 + 1/16 + ... and so on. Similarly, q is the sum from k = 1 to infinity of 1/k³, which is 1 + 1/8 + 1/27 + 1/64 + ... and continues on.\n",
            "\n",
            "The goal is to find an expression for the double sum:\n",
            "\n",
            "\\[\n",
            "\\sum_{j = 1}^\\infty \\sum_{k = 1}^\\infty \\frac{1}{(j + k)^3}\n",
            "\\]\n",
            "\n",
            "in terms of p and q. Hmm, okay. So this is a double sum over j and k, each starting from 1, and the term is 1/(j + k)³. I need to relate this to p and q.\n",
            "\n",
            "I remember that sometimes with double sums, especially symmetric ones, we can use techniques like changing variables or swapping the order of summation. Maybe I can switch the order of summation? Let me think.\n",
            "\n",
            "Let’s denote the double sum as S:\n",
            "\n",
            "\\[\n",
            "S = \\sum_{j=1}^\\infty \\sum_{k=1}^\\infty \\frac{1}{(j + k)^3}\n",
            "\\]\n",
            "\n",
            "If I swap the order of summation, it becomes:\n",
            "\n",
            "\\[\n",
            "S = \\sum_{k=1}^\\infty \\sum_{j=1}^\\infty \\frac{1}{(j + k)^3}\n",
            "\\]\n",
            "\n",
            "Which is the same as:\n",
            "\n",
            "\\[\n",
            "S = \\sum_{k=1}^\\infty \\sum_{j=1}^\\infty \\frac{1}{(k + j)^3}\n",
            "\\]\n",
            "\n",
            "But this doesn't immediately help me. Maybe I can fix a variable and see if I can express it in terms of known sums. Let me fix j and express k in terms of m, where m = j + k. Wait, but k would then be m - j, so when j goes from 1 to infinity, for each fixed m, k goes from 1 to m -\n"
          ]
        }
      ],
      "source": [
        "# TODO: Generate a response with these Math benchmark quesions\n",
        "question1 = \"How many positive whole-number divisors does 196 have?\"\n",
        "# real answer : 9\n",
        "question2 = \"What is the distance, in units, between the points $(2, -6)$ and $(-4, 3)$? Express your answer in simplest radical form.\"\n",
        "# real answer = 3\\\\sqrt{13}\n",
        "question3 = \"Define\\n\\\\[p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2} \\\\quad \\\\text{and} \\\\quad q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}.\\\\]Find a way to write\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\]in terms of $p$ and $q.$\"\n",
        "# real answer = p - q\n",
        "\n",
        "response = get_llm_response(question3)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwzkD25OFLvf"
      },
      "source": [
        "# Math Benchmark Evaluation\n",
        "\n",
        "This cell is dedicated to evaluating the performance of inference scaling methods on the Math Benchmark dataset. The process works as follows:\n",
        "\n",
        "- **Dataset Loading:** It loads the MATH-500 dataset, which contains a set of challenging math problems along with their correct solutions.\n",
        "- **Answer Extraction:** The `extract_answer` function is used to parse and extract the final answer from the generated responses. This function specifically looks for a LaTeX-style format (using `\\boxed{...}`) to reliably pinpoint the answer.\n",
        "- **Normalization and Comparison:** Before comparing, both the predicted answer and the ground truth are normalized using several functions. These functions handle different mathematical expressions, such as fractions, matrices, and algebraic expressions, ensuring that the comparison is fair and accurate regardless of formatting differences.\n",
        "- **Evaluation Loop:** For each problem:\n",
        "  - The ground truth answer is extracted from the provided solution.\n",
        "  - A response is generated by the LLM using a designated function.\n",
        "  - The predicted answer is then extracted and compared against the ground truth.\n",
        "  - The results for each problem, including whether the predicted answer is correct, are saved for later analysis.\n",
        "- **Results Analysis:** After processing all problems, the cell aggregates the results and prints a summary, including the total number of problems evaluated, the number of correct answers, and the overall accuracy.\n",
        "\n",
        "This evaluation method ensures that the output of each inference technique (such as Chain-of-Thought, Best-of-n, Beam Search, and Self-Refinement) is consistently measured against the Math Benchmark, without altering the original answers or evaluation logic.\n",
        "\n",
        "**Note:**  \n",
        "\n",
        "🔹 you don't need to modify this cell. Only rewrite the evaluation function portion then\n",
        "\n",
        "🔹 you need to run this cell before evaluating.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgnyeyBxE2Ci"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import Dict, Optional, Union\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Load the MATH-500 dataset\n",
        "def load_math500_dataset():\n",
        "    dataset = load_dataset(\"HuggingFaceH4/MATH-500\")[\"test\"]\n",
        "    return dataset\n",
        "\n",
        "# Extract the last boxed answer from text\n",
        "def extract_answer(response: str) -> Optional[str]:\n",
        "    if not response:\n",
        "        return None\n",
        "    start_idx = response.rfind('\\\\boxed{')\n",
        "    if start_idx == -1:\n",
        "        return None\n",
        "    brace_count = 1\n",
        "    pos = start_idx + 7  # length of '\\boxed{'\n",
        "    while pos < len(response) and brace_count > 0:\n",
        "        if response[pos] == '{':\n",
        "            brace_count += 1\n",
        "        elif response[pos] == '}':\n",
        "            brace_count -= 1\n",
        "        pos += 1\n",
        "    if brace_count == 0:\n",
        "        answer = response[start_idx + 7:pos - 1]\n",
        "        return answer.strip()\n",
        "    return None\n",
        "\n",
        "# Normalization and comparison functions (unchanged from original)\n",
        "def normalize_number(num_str: str) -> str:\n",
        "    try:\n",
        "        cleaned = re.sub(r'[,\\$\\\\]|\\s*(?:cm|m|kg|ft|in|lb|oz|ml|L)$|\\s*\\\\text{[^}]+}', '', num_str).strip()\n",
        "        if cleaned.startswith('.'):\n",
        "            cleaned = '0' + cleaned\n",
        "        num = float(cleaned)\n",
        "        if abs(num) < 1 and '.' in cleaned:\n",
        "            decimal_places = len(cleaned.split('.')[1])\n",
        "            format_str = f\"{{:.{decimal_places}f}}\"\n",
        "            result = format_str.format(num)\n",
        "        else:\n",
        "            result = str(num)\n",
        "        return result\n",
        "    except:\n",
        "        return num_str\n",
        "\n",
        "def numerically_equal(str1: str, str2: str) -> bool:\n",
        "    try:\n",
        "        return abs(float(str1) - float(str2)) < 1e-10\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def normalize_fraction(fraction_str: str) -> str:\n",
        "    try:\n",
        "        fraction_str = fraction_str.replace('\\\\dfrac', '\\\\frac')\n",
        "        fraction_str = ''.join(fraction_str.split())\n",
        "        fraction_str = re.sub(r'\\s*\\\\text{[^}]+}', '', fraction_str)\n",
        "        mixed_brace = re.match(r'^\\\\frac(\\d+)\\{(\\d+)\\}$', fraction_str)\n",
        "        if mixed_brace:\n",
        "            num, den = mixed_brace.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
        "        no_braces = re.match(r'^\\\\frac(\\d+)(\\d+)$', fraction_str)\n",
        "        if no_braces:\n",
        "            num, den = no_braces.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
        "        if '/' in fraction_str and not any(c in fraction_str for c in '\\\\{}'):\n",
        "            num, den = fraction_str.split('/')\n",
        "            return f\"\\\\frac{{{num.strip()}}}{{{den.strip()}}}\"\n",
        "        standard = re.match(r'^\\\\frac\\{([^{}]+)\\}\\{([^{}]+)\\}$', fraction_str)\n",
        "        if standard:\n",
        "            num, den = standard.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
        "    except:\n",
        "        return fraction_str\n",
        "\n",
        "def normalize_matrix_entry(entry: str) -> str:\n",
        "    entry = ''.join(entry.split())\n",
        "    if '/' in entry and not any(c in entry for c in '\\\\{}'):\n",
        "        if entry.startswith('-'):\n",
        "            num, den = entry[1:].split('/')\n",
        "            return f\"-{num.strip()}/{den.strip()}\"\n",
        "        else:\n",
        "            num, den = entry.split('/')\n",
        "            return f\"{num.strip()}/{den.strip()}\"\n",
        "    entry = entry.replace('\\\\dfrac', '\\\\frac')\n",
        "    frac_match = re.match(r'^(-)?\\\\frac\\{(\\d+)\\}\\{(\\d+)\\}$', entry)\n",
        "    if frac_match:\n",
        "        sign, num, den = frac_match.groups()\n",
        "        sign = sign if sign else ''\n",
        "        return f\"{sign}{num}/{den}\"\n",
        "    return entry\n",
        "\n",
        "def normalize_matrix(matrix_str: str) -> str:\n",
        "    try:\n",
        "        matrix_str = ''.join(matrix_str.split())\n",
        "        match = re.match(r'^\\\\begin\\{pmatrix\\}(.*?)\\\\end\\{pmatrix\\}$', matrix_str)\n",
        "        if not match:\n",
        "            return matrix_str\n",
        "        content = match.group(1)\n",
        "        rows = content.split('\\\\\\\\')\n",
        "        normalized_rows = []\n",
        "        for row in rows:\n",
        "            if '&' in row:\n",
        "                entries = [normalize_matrix_entry(entry) for entry in row.split('&')]\n",
        "            else:\n",
        "                entries = [normalize_matrix_entry(row)]\n",
        "            normalized_rows.append('&'.join(entries))\n",
        "        result = \"\\\\begin{pmatrix}\" + \"\\\\\\\\\".join(normalized_rows) + \"\\\\end{pmatrix}\"\n",
        "        return result\n",
        "    except:\n",
        "        return matrix_str\n",
        "\n",
        "def normalize_algebraic_expression(expr: str) -> str:\n",
        "    try:\n",
        "        expr = ''.join(expr.split())\n",
        "        monomial_match = re.match(r'^(-?\\d*\\.?\\d*)?([a-zA-Z])(?:\\^(-?\\d+))?$', expr)\n",
        "        if monomial_match:\n",
        "            coeff, var, exp = monomial_match.groups()\n",
        "            coeff = coeff if coeff and coeff not in ['+', '-'] else ('1' if not coeff else '-1')\n",
        "            exp = exp if exp else '1'\n",
        "            if coeff == '1' and exp == '1':\n",
        "                return var\n",
        "            elif coeff == '1':\n",
        "                return f\"{var}^{exp}\"\n",
        "            elif coeff == '-1' and exp == '1':\n",
        "                return f\"-{var}\"\n",
        "            elif coeff == '-1':\n",
        "                return f\"-{var}^{exp}\"\n",
        "            elif exp == '1':\n",
        "                return f\"{coeff}{var}\"\n",
        "            else:\n",
        "                return f\"{coeff}{var}^{exp}\"\n",
        "        pi_term_match = re.match(r'^(-?\\d*\\.?\\d*)\\\\?pi$', expr)\n",
        "        if pi_term_match:\n",
        "            coeff = pi_term_match.group(1)\n",
        "            if not coeff or coeff == '-':\n",
        "                coeff = '-1' if coeff == '-' else '1'\n",
        "            return f\"{coeff}\\\\pi\"\n",
        "        frac_pi_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}\\\\?pi$', expr)\n",
        "        if frac_pi_match:\n",
        "            num, den = frac_pi_match.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\\\\pi\"\n",
        "        frac_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}$', expr)\n",
        "        if frac_match:\n",
        "            num, den = frac_match.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
        "    except:\n",
        "        return expr.lower()\n",
        "\n",
        "def normalize_interval_bound(bound: str) -> str:\n",
        "    if '\\\\infty' in bound:\n",
        "        sign = '-' if bound.startswith('-') else ''\n",
        "        return f\"{sign}\\\\infty\"\n",
        "    return normalize_answer(bound) or bound\n",
        "\n",
        "def normalize_interval(interval_str: str) -> str:\n",
        "    try:\n",
        "        interval_str = ''.join(interval_str.split())\n",
        "        match = re.match(r'^\\\\left?([\\[\\(])(.*?),(.*?)\\\\right?([\\]\\)])$', interval_str)\n",
        "        if not match:\n",
        "            match = re.match(r'^([\\[\\(])(.*?),(.*?)([\\]\\)])$', interval_str)\n",
        "            if not match:\n",
        "                return interval_str\n",
        "        left_bracket, left_bound, right_bound, right_bracket = match.groups()\n",
        "        norm_left = normalize_interval_bound(left_bound)\n",
        "        norm_right = normalize_interval_bound(right_bound)\n",
        "        return f\"\\\\left{left_bracket}{norm_left},{norm_right}\\\\right{right_bracket}\"\n",
        "    except:\n",
        "        return interval_str\n",
        "\n",
        "def normalize_ordered_tuple(tuple_str: str) -> str:\n",
        "    try:\n",
        "        tuple_str = tuple_str.replace('\\\\dfrac', '\\\\frac')\n",
        "        tuple_str = tuple_str.replace('\\\\left', '').replace('\\\\right', '')\n",
        "        tuple_str = re.sub(r'\\\\?\\s+', '', tuple_str)\n",
        "        inner = tuple_str.strip('()')\n",
        "        parts = inner.split(',')\n",
        "        normalized_parts = [normalize_answer(part.strip()) for part in parts if normalize_answer(part.strip())]\n",
        "        return f\"({','.join(normalized_parts)})\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def normalize_answer(answer: str) -> str:\n",
        "    if answer is None:\n",
        "        return \"\"\n",
        "    answer = re.sub(r'\\\\text{[^}]+(?:inches|feet|meters|cm|m|kg|ft|in|lb|oz|ml|L|per|second|minute|hour)[^}]*}', '', answer)\n",
        "    answer = re.sub(r'(?<!\\\\)\\s+', '', answer)\n",
        "    ordered_pair_match = re.match(r'^(?:\\\\left)?\\((.*?)(?:\\\\right)?\\)$', answer)\n",
        "    if ordered_pair_match:\n",
        "        content = ordered_pair_match.group(1)\n",
        "        parts = content.split(',')\n",
        "        normalized_parts = [normalize_answer(part) for part in parts if normalize_answer(part)]\n",
        "        return f\"({','.join(normalized_parts)})\"\n",
        "    answer = ''.join(answer.split())\n",
        "    if not answer:\n",
        "        return None\n",
        "    pm_match = re.match(r'^(.*?)(?:\\\\pm|-)(.*?)$', answer)\n",
        "    if pm_match:\n",
        "        left, right = pm_match.groups()\n",
        "        norm_left = normalize_answer(left) if left else \"\"\n",
        "        norm_right = normalize_answer(right) if right else \"\"\n",
        "        if norm_left or norm_right:\n",
        "            return f\"{norm_left}\\\\pm{norm_right}\"\n",
        "    trig_match = re.match(r'^\\\\(?:sin|cos|tan|cot|sec|csc)\\s*([a-zA-Z])$', answer)\n",
        "    if trig_match:\n",
        "        variable = trig_match.group(1)\n",
        "        func_name = re.match(r'^\\\\(.*?)(?:\\s|$)', answer).group(1)\n",
        "        return f\"\\\\{func_name}{variable}\"\n",
        "    text_match = re.match(r'^(?:\\\\text{)?([A-Za-z]+)(?:})?$', answer)\n",
        "    if text_match:\n",
        "        return text_match.group(1).lower()\n",
        "    if (answer.startswith('\\\\left[') or answer.startswith('\\\\left(') or\n",
        "        answer.startswith('[') or answer.startswith('(')) and \\\n",
        "       (answer.endswith('\\\\right]') or answer.endswith('\\\\right)') or\n",
        "        answer.endswith(']') or answer.endswith(')')):\n",
        "        return normalize_interval(answer)\n",
        "    if answer.startswith('\\\\begin{pmatrix}') and answer.endswith('\\\\end{pmatrix}'):\n",
        "        return normalize_matrix(answer)\n",
        "    answer = answer.replace('\\\\dfrac', '\\\\frac')\n",
        "    if '\\\\frac' in answer or '/' in answer:\n",
        "        return normalize_fraction(answer)\n",
        "    neg_sqrt_match = re.match(r'^-\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
        "    if neg_sqrt_match:\n",
        "        num = neg_sqrt_match.group(1)\n",
        "        return f\"-\\\\sqrt{{{num}}}\"\n",
        "    sqrt_match = re.match(r'^(\\d*)?\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
        "    if sqrt_match:\n",
        "        coeff, num = sqrt_match.groups()\n",
        "        coeff = coeff if coeff else '1'\n",
        "        return f\"\\\\sqrt{{{num}}}\" if coeff == '1' else f\"{coeff}\\\\sqrt{{{num}}}\"\n",
        "    sqrt_with_coeff_match = re.match(r'^(\\d+)\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
        "    if sqrt_with_coeff_match:\n",
        "        coeff, num = sqrt_with_coeff_match.groups()\n",
        "        return f\"{coeff}\\\\sqrt{{{num}}}\"\n",
        "    base_match = re.match(r'^(\\d+)(?:_\\{?(\\d+)\\}?|_(\\d+))$', answer)\n",
        "    if base_match:\n",
        "        number, base1, base2 = base_match.groups()\n",
        "        base = base1 if base1 else base2\n",
        "        return f\"{number}_{base}\"\n",
        "    percent_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*\\\\?%$', answer)\n",
        "    if percent_match:\n",
        "        return normalize_number(percent_match.group(1))\n",
        "    unit_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*(?:(?:\\\\[,\\s])|,)?\\s*(?:\\\\\\\\)?(?:\\\\text{(\\w+)}|\\\\?(?:cm|m|kg|ft|in|lb|oz|ml|L))$', answer)\n",
        "    if unit_match:\n",
        "        return normalize_number(unit_match.group(1))\n",
        "    currency_match = re.match(r'^\\\\?\\$?([\\d,]+\\.?\\d*)$', answer)\n",
        "    if currency_match:\n",
        "        return normalize_number(currency_match.group(1))\n",
        "    if re.match(r'^-?[\\d,]+$', answer):\n",
        "        return normalize_number(answer)\n",
        "    unit_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:\\\\(?:mbox|text|hbox|displaystyle)\\{[^}]+\\})?(?:\\^?\\d)?$', answer)\n",
        "    if unit_match:\n",
        "        return normalize_number(unit_match.group(1))\n",
        "    mc_match = re.match(r'^\\\\text{\\(?([A-Za-z])\\)?}$|^\\(?([A-Za-z])\\)?$', answer)\n",
        "    if mc_match:\n",
        "        return (mc_match.group(1) or mc_match.group(2)).lower()\n",
        "    degree_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:(?:\\^?\\\\circ)|(?:{\\\\circ})|(?:°))?$', answer)\n",
        "    if degree_match:\n",
        "        return normalize_number(degree_match.group(1))\n",
        "    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n",
        "    try:\n",
        "        return normalize_algebraic_expression(answer)\n",
        "    except:\n",
        "        pass\n",
        "    answer = answer.replace('\\\\left', '').replace('\\\\right', '')\n",
        "    answer = answer.replace('\\\\(', '(').replace('\\\\)', ')')\n",
        "    answer = answer.replace('\\\\[', '[').replace('\\\\]', ']')\n",
        "    answer = answer.replace('\\\\{', '{').replace('\\\\}', '}')\n",
        "    answer = re.sub(r'\\\\sqrt\\{?(\\d+)\\}?', r'\\\\sqrt{\\1}', answer)\n",
        "    answer = re.sub(r'\\\\sqrt{([^{}]+)}', r'\\\\sqrt\\1', answer)\n",
        "    if re.match(r'^\\d+\\\\%$', answer) or re.match(r'^\\d+$', answer):\n",
        "        answer = re.sub(r'\\\\%$', '', answer)\n",
        "    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n",
        "    while len(answer) >= 2 and answer[0] == '{' and answer[-1] == '}':\n",
        "        if '\\\\frac' in answer:\n",
        "            break\n",
        "        answer = answer[1:-1]\n",
        "    return answer.lower() if answer else None\n",
        "\n",
        "def compare_answers(correct_answer: str, predicted_answer: Optional[str]) -> bool:\n",
        "    if predicted_answer is None:\n",
        "        return False\n",
        "    if numerically_equal(correct_answer, predicted_answer):\n",
        "        return True\n",
        "    normalized_correct = normalize_answer(correct_answer)\n",
        "    normalized_predicted = normalize_answer(predicted_answer)\n",
        "    if not normalized_correct or not normalized_predicted:\n",
        "        return False\n",
        "    if normalized_correct == \"\" and normalized_predicted == \"\":\n",
        "        return False\n",
        "    if ('\\\\left[' in normalized_correct or '\\\\left(' in normalized_correct) and \\\n",
        "       ('\\\\left[' in normalized_predicted or '\\\\left(' in normalized_predicted):\n",
        "        return normalized_correct == normalized_predicted\n",
        "    return normalized_correct == normalized_predicted\n",
        "\n",
        "# Load existing results\n",
        "def load_existing_results(filename: str) -> list[Dict]:\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "# Save a single result\n",
        "def save_result(filename: str, result: Dict):\n",
        "    results = load_existing_results(filename)\n",
        "    results.append(result)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "# Analyze and print results\n",
        "def analyze_results(results: list[Dict]):\n",
        "    total = len(results)\n",
        "    correct = sum(1 for r in results if r['is_correct'])\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(\"\\n=== Results Summary ===\")\n",
        "    print(f\"Total problems: {total}\")\n",
        "    print(f\"Correct answers: {correct}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(\"\\n=== Incorrect Problems ===\")\n",
        "    for r in results:\n",
        "        if not r['is_correct']:\n",
        "            print(f\"Problem {r['index']}:\")\n",
        "            print(f\"Expected: {r['correct_answer']}\")\n",
        "            print(f\"Predicted: {r['predicted_answer']}\")\n",
        "            print(\"---\")\n",
        "\n",
        "# Main evaluation function\n",
        "def evaluate():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    t=0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes:\n",
        "            continue\n",
        "        t += 1\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])  # Extract from 'solution', not 'answer'\n",
        "        response = get_llm_response(problem_text)\n",
        "        predicted_answer = extract_answer(response)\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"cnt :  {cnt} idx: {t}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNUWF2U_HC8g"
      },
      "source": [
        "# Customizable CoT Prompt Template\n",
        "* modify cot prompt then evaluate on math benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXabvwusFLvg"
      },
      "outputs": [],
      "source": [
        "# final answer should be in this format: (because of extract_answer function you can change it if you want)\n",
        "#\\\\[\n",
        "#\\\\boxed{your_answer_here}\n",
        "#\\\\]\n",
        "\n",
        "COT_PROMPT = \"\"\"You are an intelligent assistant to solve mathmetical problems. Whenever you face a problem, you have the task to solve it. You should solve the problems step by step. First, break the initial problem into simpler sub problems and then solve each one and finally solve the original problem.\n",
        "\n",
        "Important: Always end your solution with the final answer in this format:\n",
        "\n",
        "\\\\[\n",
        "\\\\boxed{your_answer_here}\n",
        "\\\\]\n",
        "\n",
        "Here is the problem you should solve:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_4to2EjFLvg"
      },
      "source": [
        "* generate response with cot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58w5yeo9GrP2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_COT_response(problem):\n",
        "    prompt = COT_PROMPT + \"\\n\" + problem\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "\n",
        "        ],\n",
        "    \"max_tokens\": 1900,\n",
        "    \"temperature\": 0.3\n",
        "    }\n",
        "    response = requests.post(url, json=payload)\n",
        "    return response.json()['choices'][0]['message']['content'].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Una6ucDAHb2_"
      },
      "source": [
        "# Evaluate CoT\n",
        "* modify response generation part to evalute this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na6kksqZHbOq"
      },
      "outputs": [],
      "source": [
        "def evaluate_cot():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek_cot.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes:\n",
        "            continue\n",
        "        if idx >= 10:\n",
        "          break\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])\n",
        "\n",
        "        # TODO: Generate a response with cot\n",
        "        response = get_COT_response(problem_text)\n",
        "        predicted_answer = extract_answer(response)\n",
        "        ##########################################################\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJbLqPXYOLvm",
        "outputId": "09758bff-098d-48d7-feaf-ff33614d9443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   2%|▏         | 10/500 [00:00<00:00, 5750.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Results Summary ===\n",
            "Total problems: 10\n",
            "Correct answers: 4\n",
            "Accuracy: 40.00%\n",
            "\n",
            "=== Incorrect Problems ===\n",
            "Problem 1:\n",
            "Expected: p - q\n",
            "Predicted: None\n",
            "---\n",
            "Problem 3:\n",
            "Expected: 9\n",
            "Predicted: None\n",
            "---\n",
            "Problem 4:\n",
            "Expected: \\text{Evelyn}\n",
            "Predicted: None\n",
            "---\n",
            "Problem 6:\n",
            "Expected: 27\n",
            "Predicted: None\n",
            "---\n",
            "Problem 7:\n",
            "Expected: 90^\\circ\n",
            "Predicted: None\n",
            "---\n",
            "Problem 9:\n",
            "Expected: 4\n",
            "Predicted: None\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_cot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vntdIRYQFLvh"
      },
      "source": [
        "## Best-of-N\n",
        "\n",
        "The Best-of-N approach generates several candidate responses for a problem and then selects the one with the highest average token log-likelihood. This ensures that the final answer, formatted within the `\\boxed{}` command, is not only correct in presentation but also statistically the most reliable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NylbVYkT1kx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "SYSTEM_PROMPT = '''You are solving mathematics problems.\n",
        "\n",
        "Please think step by step.\n",
        "\n",
        "Important: Always end your solution with the final answer in this format:\n",
        "\n",
        "\\\\[\n",
        "\\\\boxed{your_answer_here}\n",
        "\\\\]\n",
        "\n",
        "The entire answer should be contained completely within the \\\\boxed{} command.'''\n",
        "\n",
        "\n",
        "\n",
        "def best_of_n_response(problem, N=5):\n",
        "    best_answer = None\n",
        "    best_avg_likelihood = float('-inf')\n",
        "    best_responses = []\n",
        "    prompt = SYSTEM_PROMPT + \"\\n\" + problem\n",
        "\n",
        "    for t in range(N):\n",
        "\n",
        "        # TODO: Generate a response\n",
        "        url = \"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "        payload = {\n",
        "            \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "\n",
        "            ],\n",
        "        \"max_tokens\": 1900,\n",
        "        \"temperature\": 0.3,\n",
        "        \"logprobs\": True\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, json=payload)\n",
        "        response = response.json()\n",
        "\n",
        "        # TODO:  Iterate over each choice in the response and append lobprob of each tocken to token_logprobs (you can see a sample of response to see how to extract the token logprobs)\n",
        "        response_text = response['choices'][0]['message']['content']\n",
        "        answer = extract_answer(response_text)\n",
        "        token_logprobs = []\n",
        "\n",
        "        for token_logprob in response['choices'][0]['logprobs']['content']:\n",
        "          token_logprobs.append(token_logprob['logprob'])\n",
        "\n",
        "        # TODO: Calculate the average log-likelihood and store the response, answer(that is extracted with extract_answer()), and average log-likelihood\n",
        "        mean_logprob = np.mean(token_logprobs)\n",
        "        best_responses.append((response_text, answer, mean_logprob))\n",
        "\n",
        "\n",
        "    # TODO: Group the responses by the answer (multiple responses can have the same answer)\n",
        "    group_dict = {}\n",
        "    for r, a, s in best_responses:\n",
        "      if a not in group_dict:\n",
        "        group_dict[a] = []\n",
        "\n",
        "      group_dict[a].append(s)\n",
        "\n",
        "    # TODO: Find the best answer based on the average likelihood\n",
        "    for a in group_dict:\n",
        "      avg_likelihood = np.mean(group_dict[a])\n",
        "      if avg_likelihood > best_avg_likelihood:\n",
        "        best_avg_likelihood = avg_likelihood\n",
        "        best_answer = answer\n",
        "\n",
        "    # return best_answer\n",
        "    return best_answer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_of_n_response(question3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KyYGSn473-iZ",
        "outputId": "8923c1ed-bca9-4afc-ff4d-8138686f56c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'p - q'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8msdz_ihFLvh"
      },
      "source": [
        "# Evaluate best of n\n",
        "\n",
        "* modify response generation part to evalute this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyCkE9ToOHNO"
      },
      "outputs": [],
      "source": [
        "def evaluate_best_of_n():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek_best_of_n.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes:\n",
        "            continue\n",
        "        if idx >= 10:\n",
        "          break\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])\n",
        "        # TODO: ##########################################################\n",
        "        response = best_of_n_response(problem_text)\n",
        "        predicted_answer = response\n",
        "        ##########################################################\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_6D_Pb8V3tr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705,
          "referenced_widgets": [
            "2be5c26c6c9e40d48b77b068ecae90e4",
            "7f54b39cf8ba4a02ad9797a92a11d3fb",
            "45892f31e1ad43f5b1b0519088a3ca89",
            "a43084754d854912a156b02e293ee276",
            "99b4919a604f4e3aaf87cbbd24709818",
            "499d6c15224641948c1ca5bf18d548aa",
            "47a2598c72304c899402da0d3da8473a",
            "67260fcca1414dd58ea6a758737d39b4",
            "0225ff670afb4a5d84c92756ab5a07ec",
            "762fa159570e457f9777dce44ea358a4",
            "2acbf422fd3c46d081a7ef94b6209aed",
            "cfea52494976476b91c97a7115507e4b",
            "a09c29455e184335863a17904a06a828",
            "db6026c8677a453aafb5c81fe863691e",
            "9bbe64615e324501b5a884f9a6482ef2",
            "fba93e38eb3d45aea1db7f1b37a4d22f",
            "8235316fdbf442db9a004f162074a37d",
            "c46f4f449a8c4428aecf624fc8459d22",
            "ce1ce8fe0dc34121b5d47979f717d855",
            "27b52da18f7a44ee940d71fc40ec5d82",
            "ef087039c4fb4c568ab883772096047d",
            "0e2654e61fd1430da765f2e7fcf6d203",
            "7fb5824dfc7540b0b7ab42b4c9c70382",
            "6a8846f857c34419a6ffcd85fbb9c367",
            "7e04b7d32fc643abb972dc051e4d6d4d",
            "bf4219daa33e4ec1a4dd57aa28954f80",
            "ba7598ddf71646dcb1045dd3a62a48a9",
            "0a457886d44b43649ddde9d3ece462eb",
            "bd4daa7d9f2e414aac94f1904f58f8ff",
            "97d7df36850e4233a1681000dc925c9a",
            "4a2b0bfed3c24e918a4f45fa5b2a94ae",
            "2f5dddd30e1d4bbc9107d1ab27c5fb82",
            "a2763f34d0fc47349c17ef287d53e7f6"
          ]
        },
        "outputId": "bbce6718-d0d9-481d-b5c6-05309dba3a16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/412 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2be5c26c6c9e40d48b77b068ecae90e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.jsonl:   0%|          | 0.00/447k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfea52494976476b91c97a7115507e4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fb5824dfc7540b0b7ab42b4c9c70382"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   0%|          | 1/500 [00:43<5:58:18, 43.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  1 idx: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   0%|          | 2/500 [03:35<16:27:19, 118.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  1 idx: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 3/500 [04:31<12:27:34, 90.25s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  2 idx: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 4/500 [05:18<10:05:00, 73.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 5/500 [06:34<10:13:01, 74.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  4 idx: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 6/500 [07:08<8:18:56, 60.60s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  5 idx: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|▏         | 7/500 [09:47<12:42:10, 92.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  6 idx: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   2%|▏         | 8/500 [12:37<16:03:25, 117.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  7 idx: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   2%|▏         | 9/500 [13:57<14:23:18, 105.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  8 idx: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   2%|▏         | 10/500 [16:48<13:43:41, 100.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  8 idx: 9\n",
            "\n",
            "=== Results Summary ===\n",
            "Total problems: 10\n",
            "Correct answers: 8\n",
            "Accuracy: 80.00%\n",
            "\n",
            "=== Incorrect Problems ===\n",
            "Problem 1:\n",
            "Expected: p - q\n",
            "Predicted: None\n",
            "---\n",
            "Problem 9:\n",
            "Expected: 4\n",
            "Predicted: None\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_best_of_n()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_nsTQ8EFLvh"
      },
      "source": [
        "## Beam Search\n",
        "\n",
        "This cell implements a beam search strategy for generating candidate reasoning chains. The method generates multiple continuations at each reasoning step, scoring each candidate based on its average token log-likelihood. By retaining and expanding only the top candidates, the approach efficiently searches for the most promising chain-of-thought that leads to the final answer in the required format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2ZQr1A0WBD0"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = '''You are solving mathematics problems.\n",
        "\n",
        "Please think regarding each step provided here.\n",
        "\n",
        "Important: Always end your solution with the final answer in this format:\n",
        "\n",
        "\\\\[\n",
        "\\\\boxed{your_answer_here}\n",
        "\\\\]\n",
        "\n",
        "The entire answer should be contained completely within the \\\\boxed{} command.'''\n",
        "\n",
        "def call_qwen_model_raw(prompt,step_num, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Sends a request to the local Qwen endpoint and returns the generated text\n",
        "    along with the average token log-probability.\n",
        "    \"\"\"\n",
        "    # Build the prompt. We assume the sample already contains the SYSTEM_PROMPT. you can modify max_tokens for different steps\n",
        "    # TODO: Send a request to the Qwen model and get the response\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "\n",
        "        ],\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.3,\n",
        "    \"logprobs\": True\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload)\n",
        "    response = response.json()\n",
        "\n",
        "    # TODO:  Iterate over each choice in the response and append lobprob of each tocken to token_logprobs\n",
        "\n",
        "    token_logprobs = [token_logprob['logprob'] for token_logprob in response['choices'][0]['logprobs']['content']]\n",
        "\n",
        "    # TODO: Calculate the average log-likelihood and store the response, answer(that is extracted with extract_answer()), and average log-likelihood\n",
        "    avg_token_prob = np.mean(token_logprobs)\n",
        "\n",
        "    output_text = response['choices'][0]['message']['content']\n",
        "    return output_text, avg_token_prob, len(token_logprobs)\n",
        "\n",
        "\n",
        "\n",
        "class BeamCandidate:\n",
        "    def __init__(self, sequence, cumulative_log_prob, step_scores, finished=False,num_token = 0):\n",
        "        self.sequence = sequence\n",
        "        self.cumulative_log_prob = cumulative_log_prob\n",
        "        self.step_scores = step_scores\n",
        "        self.finished = finished\n",
        "        self.num_token = num_token\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"BeamCandidate(score={self.cumulative_log_prob:.3f}, finished={self.finished}, \"\n",
        "                f\"sequence={self.sequence})\")\n",
        "\n",
        "\n",
        "\n",
        "def generate_reasoning_steps(context, step_num, top_k):\n",
        "    \"\"\"\n",
        "    For a given candidate reasoning chain (context), generate top_k candidate continuations\n",
        "    for the current reasoning step (from 1 to 5). Each candidate is verified using the average\n",
        "    token logprob as a proxy for quality.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: each step should have a different prompt and the prompt should be added to the context so make a prompt for each step that explains what the step is about\n",
        "    candidates = []\n",
        "    for i in range(top_k):\n",
        "        if step_num == 1:\n",
        "            candidate_prompt = context + \"\\nStep 1: What assumptions or facts are needed to solve the problem?\"\n",
        "        elif step_num == 2:\n",
        "            candidate_prompt = context + \"\\nStep 2: Use the above to start solving. Show relevant calculations or logic.\"\n",
        "        elif step_num == 3:\n",
        "            candidate_prompt = context + \"\\nStep 3: Conclude the reasoning and give the final answer clearly.\"\n",
        "\n",
        "\n",
        "        # TODO: call the qwen model to get the output and avg_token_prob\n",
        "        output_text, avg_token_prob, num_token = call_qwen_model_raw(candidate_prompt, step_num)\n",
        "        candidate_step = output_text.strip()\n",
        "        finished = True if extract_answer(candidate_step) is not None else False\n",
        "\n",
        "        candidates.append((candidate_step, avg_token_prob,num_token, finished))\n",
        "\n",
        "    return candidates\n",
        "\n",
        "def beam_search(init_problem_prompt, beam_width=3, max_steps=3, top_k=2):\n",
        "    \"\"\"\n",
        "    Implements a beam search over reasoning steps.\n",
        "    \"\"\"\n",
        "    prompt = init_problem_prompt\n",
        "    initial_candidate = BeamCandidate(prompt, 0, [])\n",
        "    beams = [initial_candidate]\n",
        "\n",
        "\n",
        "    for step_num in range(1, max_steps+1):\n",
        "        new_beams = []\n",
        "\n",
        "        for candidate in beams:\n",
        "            if candidate.finished:\n",
        "                # TODO: Propagate finished candidates unchanged.\n",
        "                new_beams.append(candidate)\n",
        "                continue\n",
        "            step_candidates = generate_reasoning_steps(candidate.sequence, step_num, top_k)\n",
        "            for (step_text, score,num_token, finished) in step_candidates:\n",
        "                # TODO: Create a new candidate by appending the step text to the current sequence and updating the log-probability by averaging all token_logprobs after the new step.\n",
        "                new_text = candidate.sequence + '\\n' + step_text\n",
        "                new_step_scores = candidate.step_scores.copy()\n",
        "                new_step_scores.append(score)\n",
        "                new_cumulative_log_prob = np.mean(new_step_scores)\n",
        "                new_num_token = candidate.num_token + num_token\n",
        "\n",
        "                new_candidate = BeamCandidate(new_text,\n",
        "                                              new_cumulative_log_prob,\n",
        "                                              new_step_scores,\n",
        "                                              finished,\n",
        "                                              new_num_token)\n",
        "                new_beams.append(new_candidate)\n",
        "\n",
        "        if not new_beams:\n",
        "            break\n",
        "        # TODO: sort the new beams based on the cumulative_log_prob and put the top beam_width beams in the beams list\n",
        "        beams = sorted(new_beams, key=lambda x:x.cumulative_log_prob, reverse=True)\n",
        "        beams = beams[:beam_width]\n",
        "\n",
        "\n",
        "        if all(beam.finished for beam in beams):\n",
        "            break\n",
        "    # TODO: Get the best candidate from the beams list that is finished\n",
        "    finished_beams = sorted(beams, key=lambda x:x.cumulative_log_prob, reverse=True)\n",
        "\n",
        "    best_candidate = beams[0]\n",
        "    return best_candidate\n",
        "\n",
        "def run_qwen_beam_search(problem,beam_width, max_steps, top_k, log_level):\n",
        "    \"\"\"\n",
        "    sets up the sample prompt, performs beam search,\n",
        "    and extracts the final answer.\n",
        "    \"\"\"\n",
        "    # TODO: Set the initial prompt to the problem and run the beam search to get the best candidate\n",
        "    init_problem_prompt = SYSTEM_PROMPT + '\\n' + problem\n",
        "    best = beam_search(init_problem_prompt, beam_width, max_steps, top_k)\n",
        "\n",
        "    if log_level > 0:\n",
        "      print(\"Final sequence:\\n\", best.sequence)\n",
        "      print(\"Log Prob:\", best.cumulative_log_prob)\n",
        "\n",
        "    final_answer = best.sequence\n",
        "    return final_answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3YRuWGMFLvi"
      },
      "source": [
        "# Evaluate beam search\n",
        "* modify response generation part to evalute this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR_TyvGMazTc"
      },
      "outputs": [],
      "source": [
        "def evaluate_beam_search():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek_beam_search.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes :\n",
        "            continue\n",
        "        if idx >= 10:\n",
        "          break\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])\n",
        "        ##########################################################\n",
        "        # TODO: Generate a response with beam search\n",
        "        response = run_qwen_beam_search(problem_text, 3, 3, 2, 0)\n",
        "        predicted_answer = extract_answer(response)\n",
        "        ##########################################################\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxMjVBf6bpiy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847,
          "referenced_widgets": [
            "e5406c2cd2154172aa2797e5c9987cc2",
            "e0837f33ab53463a959729de57222015",
            "1b7c92d1913b4241b6b10bdd8738c354",
            "286de0a2ec774e82bb537ab989c96c95",
            "cd96e94b84df4abd86dc2c889cfa3c78",
            "844e222daba1481396363d9d214a5266",
            "e9879f0cf1174633a88b6cedda10f873",
            "6af11a29c86c401a802a068152d48079",
            "6abea01cd255435b990ca17507fb4c3e",
            "4ad413a211f741b490858f9afaf182ad",
            "d597f98dd5b24e069fbd7b79e6272183",
            "286f1ea17e2f429d95476fac9fe3af79",
            "0189bf64f5f84988af5428c095757ff4",
            "1bfb0adeff664795858c952af063814a",
            "b96f17f909fa4532a232deddc8cc948c",
            "983c545b4aac462797b2f3975ce98f6b",
            "40544865d289438c9dce61fbc2c4d86c",
            "c8fc4a20d51e4020aaf68c2910a98191",
            "2ddba8b63d644abb83f93ea03904eb9d",
            "e852b563c9a94486bacabcabd6e218a4",
            "267a9fb61f6f497a9bfda725275fd043",
            "f8d53d20a00c4e45a183832604b8edef",
            "ecb90d4131344f1a8e04ccaf5434f2ab",
            "567735bc40a44c31945ac337a9928dfc",
            "30eda6c38d2c406e9de0a0dd91e9ba3f",
            "28cec9977ce2448790c20c2102050bb6",
            "d46795c136b947dfb875547c76143463",
            "abf1600790b94f9c9226a663fae23f66",
            "da1bc1bdecd340bd883dec01ca9ddef2",
            "7f671ce8f1af4065a9a2f71b8e1011c6",
            "47f54fee017745f493489af0d1fd7700",
            "38305d58123a49628c7ac75aec288094",
            "af290567c48c4a80be4e759d2fe8cf71"
          ]
        },
        "outputId": "2e3fb614-a4ea-4cc7-aad1-d22c3b9b930a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/412 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5406c2cd2154172aa2797e5c9987cc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.jsonl:   0%|          | 0.00/447k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "286f1ea17e2f429d95476fac9fe3af79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecb90d4131344f1a8e04ccaf5434f2ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   0%|          | 1/500 [00:32<4:32:17, 32.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  1 idx: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   0%|          | 2/500 [03:31<16:24:57, 118.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  1 idx: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 3/500 [05:37<16:51:49, 122.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  2 idx: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 4/500 [06:48<14:01:49, 101.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 5/500 [09:01<15:33:22, 113.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 6/500 [09:34<11:46:26, 85.80s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  4 idx: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|▏         | 7/500 [11:36<13:23:20, 97.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  5 idx: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   2%|▏         | 8/500 [15:05<18:11:36, 133.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  5 idx: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   2%|▏         | 9/500 [16:21<15:42:09, 115.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  6 idx: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   2%|▏         | 10/500 [20:01<16:21:13, 120.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  6 idx: 9\n",
            "\n",
            "=== Results Summary ===\n",
            "Total problems: 10\n",
            "Correct answers: 6\n",
            "Accuracy: 60.00%\n",
            "\n",
            "=== Incorrect Problems ===\n",
            "Problem 1:\n",
            "Expected: p - q\n",
            "Predicted: \n",
            "---\n",
            "Problem 4:\n",
            "Expected: \\text{Evelyn}\n",
            "Predicted: \n",
            "---\n",
            "Problem 7:\n",
            "Expected: 90^\\circ\n",
            "Predicted: \n",
            "---\n",
            "Problem 9:\n",
            "Expected: 4\n",
            "Predicted: \n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_beam_search()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcctBdPbFLvi"
      },
      "source": [
        "## Self-Refinement\n",
        "\n",
        "This approach begins by generating an initial solution using the given prompt. It then iteratively refines this output by providing the model with targeted feedback and asking it to improve its response. The process continues until the feedback indicates that no further refinement is necessary, ensuring that the final answer—properly formatted within the `\\boxed{}` command—is as accurate and well-reasoned as possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FEEDBACK_PROMPT = \"\"\"In this message, you are give a prompt for solving a mathematical problem and an answer to this question. Analyze the answer and determine that is there any issue in the proposed solution and answer or not.\n",
        "        In the case that there is something wrong with the answer, explain it completely. Your output should be in this format:\n",
        "        **answer_status**: OK if answer is completely correct, WRONG if there is something incorrect with the soluion\n",
        "        **explanation_for_wrong**: write nothing if answer_status is OK, else provide explanation and feedback on the issue found in the solution\n",
        "        \"\"\"\n",
        "\n",
        "prompt = SYSTEM_PROMPT + \"\\n\" + question3\n",
        "\n",
        "current_output = generate_content(prompt)\n",
        "\n",
        "FEEDBACK_PROMPT = \"\"\"In this message, you are give a prompt for solving a mathematical problem and an answer to this question. Analyze the answer and determine that is there any issue in the proposed solution and answer or not.\n",
        "In the case that there is something wrong with the answer, explain it completely. Your output should be in this format:\n",
        "**answer_status**: OK if answer is completely correct, WRONG if there is something incorrect with the soluion\n",
        "**explanation_for_wrong**: write nothing if answer_status is OK, else provide explanation and feedback on the issue found in the solution\n",
        "\"\"\"\n",
        "feedback_prompt = FEEDBACK_PROMPT + '\\n' + 'PROMPT AND PROBLEM:\\n' + prompt + 'ANSWER:\\n' + current_output\n",
        "\n",
        "# TODO: Send the feedback prompt to the model using generate_content and capture the feedback response.\n",
        "output = generate_content(feedback_prompt)\n",
        "\n",
        "output.split('**explanation_for_wrong**:')[1]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "gQcYsQBojod8",
        "outputId": "bc2a4843-496d-4fbe-a705-5b2fb5581524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, so I need to find a way to express the double sum \\\\(\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\) in terms of \\\\(p\\\\) and \\\\(q\\\\), where \\\\(p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2}\\\\) and \\\\(q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}\\\\).\\n\\nFirst, I recognize that the double sum involves terms where \\\\(j\\\\) and \\\\(k\\\\) are positive integers, and each term is the reciprocal of the cube of their sum. I want to relate this to the known sums \\\\(p\\\\) and \\\\(q\\\\).\\n\\nI decide to change the order of summation. Instead of summing over \\\\(j\\\\) and \\\\(k\\\\) separately, I consider the sum over all possible sums \\\\(n = j + k\\\\). For each \\\\(n\\\\), \\\\(j\\\\) can range from 1 to \\\\(n - 1\\\\), and \\\\(k = n - j\\\\). So, the double sum can be rewritten as a single sum over \\\\(n\\\\) from 2 to infinity, with each term being the sum over \\\\(j\\\\) from 1 to \\\\(n - 1\\\\) of \\\\(\\\\frac{1}{n^3}\\\\).\\n\\nThis gives me:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\sum_{j = 1}^{n - 1} \\\\frac{1}{n^3}\\n\\\\]\\n\\nSince the inner sum is just adding \\\\(\\\\frac{1}{n^3}\\\\) a total of \\\\(n - 1\\\\) times, it simplifies to:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\frac{n - 1}{n^3}\\n\\\\]\\n\\nWhich can be rewritten as:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\left( \\\\frac{1}{n^2} - \\\\frac{1}{n^3} \\\\right )\\n\\\\]\\n\\nBreaking this down, the sum becomes:\\n\\n\\\\[\\n\\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^2} - \\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^3}\\n\\\\]\\n\\nI know that \\\\(p = \\\\sum_{n = 1}^\\\\infty \\\\frac{1}{n^2}\\\\), so subtracting the first term (when \\\\(n = 1\\\\)) gives me \\\\(\\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^2} = p - 1\\\\). Similarly, \\\\(q = \\\\sum_{n = 1}^\\\\infty \\\\frac{1}{n^3}\\\\), so subtracting the first term gives \\\\(\\\\sum_{n = 2}^\\\\infty \\\\frac{1}{n^3} = q - 1\\\\).\\n\\nSubstituting these back into the expression, I get:\\n\\n\\\\[\\n(p - 1) - (q - 1) = p - 1 - q + 1 = p - q\\n\\\\]\\n\\nSo, the double sum simplifies to \\\\(p - q\\\\). Therefore, the answer is \\\\(p - q\\\\).\\n\\nWait, but let me double-check. When I changed the order of summation, I considered \\\\(n = j + k\\\\), which is correct. Then, for each \\\\(n\\\\), the inner sum is indeed \\\\(\\\\sum_{j=1}^{n-1} \\\\frac{1}{n^3}\\\\), which is \\\\(\\\\frac{n - 1}{n^3}\\\\). Then, summing over \\\\(n\\\\) from 2 to infinity, I split it into two sums: one over \\\\(\\\\frac{1}{n^2}\\\\) and one over \\\\(\\\\frac{1}{n^3}\\\\). Subtracting the \\\\(n=1\\\\) terms, which are 1 and 1 respectively, gives me \\\\(p - 1 - (q - 1)\\\\), which simplifies to \\\\(p - q\\\\). That seems correct.\\n\\nI don't see any errors in this reasoning. So, the final answer is \\\\(p - q\\\\).\\n</think>\\n\\n**answer_status**: OK\\n\\n**explanation_for_wrong**: There is no issue with the solution. The reasoning is correct and the final answer is \\\\(p - q\\\\).\\n\\n\\\\[\\n\\\\boxed{p - q}\\n\\\\]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.split()"
      ],
      "metadata": {
        "id": "2NbHoxXSkfI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAYp7rCbiJQa"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = '''You are solving mathematics problems.\n",
        "\n",
        "Please think step by step.\n",
        "\n",
        "Important: Always end your solution with the final answer in this format:\n",
        "\n",
        "\\\\[\n",
        "\\\\boxed{your_answer_here}\n",
        "\\\\]\n",
        "\n",
        "The entire answer should be contained completely within the \\\\boxed{} command.'''\n",
        "\n",
        "\n",
        "\n",
        "def generate_content(prompt):\n",
        "\n",
        "    # TODO: Send a request to the Qwen model and get the response\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "\n",
        "        ],\n",
        "    \"max_tokens\": 1500,\n",
        "    \"temperature\": 0.3,\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload)\n",
        "    response = response.json()\n",
        "    output_text = response['choices'][0]['message']['content']\n",
        "\n",
        "    return output_text\n",
        "\n",
        "def self_refine(problem, max_iter=2):\n",
        "\n",
        "    prompt = SYSTEM_PROMPT + \"\\n\" + problem\n",
        "\n",
        "    # TODO: Generate the initial output using generate_content with the full prompt.\n",
        "    current_output = generate_content(prompt)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        # TODO: Provide a feedback prompt that asks the model to analyze current_output.\n",
        "        #       - Include both the original prompt and the current output\n",
        "        #       - speicfy the format if the feedback is needed so you can parse it later\n",
        "\n",
        "        feedback_prompt = f\"\"\"You are a mathematical assistant analyzing a solution.\n",
        "        Original problem:\n",
        "        {problem}\n",
        "\n",
        "        Current solution:\n",
        "        {current_output}\n",
        "\n",
        "        Your task:\n",
        "        1. Point out any mistakes or unclear steps in the solution.\n",
        "        2. If the solution is correct, say: \"Solution is correct. No changes needed.\"\n",
        "        3. Otherwise, suggest how to refine the solution.\n",
        "        Be concise and use this format:\n",
        "\n",
        "        FEEDBACK:\n",
        "        <your feedback here>\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Send the feedback prompt to the model using generate_content and capture the feedback response.\n",
        "        feedback = generate_content(feedback_prompt)\n",
        "\n",
        "        # TODO: Parse the feedback response to determine if refinement is needed.\n",
        "\n",
        "        if \"Solution is correct\" in feedback:\n",
        "          break\n",
        "\n",
        "        # TODO: If refinement is needed:\n",
        "        #       - Create a refine prompt that includes the original prompt, current output, and the feedback.\n",
        "        #       - Send this refine prompt to the model using generate_content to obtain a refined output.\n",
        "        #       - Update current_output with the refined output.\n",
        "        refine_prompt = f\"\"\"You're a mathematics assistant helping improve a student's solution.\n",
        "        Original problem:\n",
        "        {problem}\n",
        "\n",
        "        Previous solution:\n",
        "        {current_output}\n",
        "\n",
        "        Feedback from review:\n",
        "        {feedback}\n",
        "\n",
        "        Please provide a corrected and refined solution following this format:\n",
        "\n",
        "        \\\\[\n",
        "        \\\\boxed{{your_answer_here}}\n",
        "        \\\\]\n",
        "        \"\"\"\n",
        "\n",
        "        current_output = generate_content(refine_prompt)\n",
        "\n",
        "    # TODO: Extract the final answer from current_output (e.g., using an extract_answer function).\n",
        "    answer = extract_answer(current_output)\n",
        "    return answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2wSgT37FLvi"
      },
      "source": [
        "# Evaluate Self-Refinement\n",
        "* modify response generation part to evalute this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln5I9pg_jN7Z"
      },
      "outputs": [],
      "source": [
        "def evaluate_self_refiner():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek_self_refiner.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes :\n",
        "            continue\n",
        "        if idx >= 10:\n",
        "          break\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])\n",
        "        ##########################################################\n",
        "        # TODO: Generate a response with self_refine\n",
        "        response = self_refine(problem_text)\n",
        "        predicted_answer = response\n",
        "        ##########################################################\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_self_refiner()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPT3tG5mL9MG",
        "outputId": "50fc0a94-5b72-4cc8-e15b-8926b0d31640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   0%|          | 1/500 [00:20<2:52:14, 20.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  1 idx: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   0%|          | 2/500 [01:07<5:00:08, 36.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  1 idx: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 3/500 [02:02<6:09:32, 44.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  2 idx: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 4/500 [03:10<7:26:47, 54.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 5/500 [04:28<8:35:02, 62.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 6/500 [05:06<7:25:51, 54.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|▏         | 7/500 [06:32<8:50:27, 64.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  4 idx: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   2%|▏         | 8/500 [07:49<9:21:53, 68.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  5 idx: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   2%|▏         | 9/500 [08:35<8:23:55, 61.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  6 idx: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   2%|▏         | 10/500 [10:47<8:49:06, 64.79s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  6 idx: 9\n",
            "\n",
            "=== Results Summary ===\n",
            "Total problems: 10\n",
            "Correct answers: 6\n",
            "Accuracy: 60.00%\n",
            "\n",
            "=== Incorrect Problems ===\n",
            "Problem 1:\n",
            "Expected: p - q\n",
            "Predicted: None\n",
            "---\n",
            "Problem 4:\n",
            "Expected: \\text{Evelyn}\n",
            "Predicted: your_answer_here\n",
            "---\n",
            "Problem 5:\n",
            "Expected: 42\n",
            "Predicted: 42\\ \\text{inches}\n",
            "---\n",
            "Problem 9:\n",
            "Expected: 4\n",
            "Predicted: None\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNAg9wa9jlSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919,
          "referenced_widgets": [
            "5b02d2b7fca045558485b128ea54f947",
            "7ab44b8b89084e64afa54e795bfd4ead",
            "93075539ee1545e3830d1af7ae64b641",
            "d94cf06e2d1f43e796992896fbd17933",
            "3a8f7faa13074d8d862d6d727d509d9f",
            "126f2dac817f48bb971d94b5be7a220d",
            "c3377123649f43dc81f728d91824cffb",
            "2fa6d16ed7114ff8b3fba682c8d67c7e",
            "a8e959d08263459c8fffb922c31a0e06",
            "9271ddc83a46439b8e5a6730d52052c0",
            "e4bf36df1a824bd98aa59d100bb13578",
            "2189a88b16ac4a94b726445c2420feeb",
            "f2fc0d1c048b4f6186680a429c7b6980",
            "869f0ada92a24619882392c5b084f994",
            "7e997598a48b4db6a5475f4d306456e9",
            "cfb6c2a159c44058b2b087875dd952f3",
            "61a60fb35bf3494db58990be024309f3",
            "364049304ed340baa24843c270a4b7f2",
            "a1b632d9be7348f784df0d4b10ae9010",
            "3058efaa40954732b0755a08e46eda9f",
            "239c519be75d4efb964899f01e1b4748",
            "2d08afc27f914036a30a3c06d6ea7bf9",
            "d67d1731d2f34063aa2cb51c39089f54",
            "ccbbcb77baa4460ea0fad800c4e915f2",
            "e9fa243f73c84821a8dcc6146f8b1815",
            "e6aaca4042e0427ebadc413cc10637fc",
            "6cf63adeea144de29bb332709efb224c",
            "3ae5d91cd9f748beb49ac6256f36f9a5",
            "8b7da10d996748729f5171322a9f391b",
            "4a7293bc156f479c91b17b6c0c23f167",
            "47a5d84e1a18400881d14f658f596f46",
            "c5684f8b0f7c4b8bbccc2fa9cecf4ad0",
            "13ed151f6a0b45a28e942649934a9d44"
          ]
        },
        "outputId": "c66a0625-eaf2-427d-ae09-5c4bf3946629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/412 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b02d2b7fca045558485b128ea54f947"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.jsonl:   0%|          | 0.00/447k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2189a88b16ac4a94b726445c2420feeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d67d1731d2f34063aa2cb51c39089f54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   0%|          | 1/500 [00:31<4:19:14, 31.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  0 idx: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   0%|          | 2/500 [01:52<8:21:29, 60.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  0 idx: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 3/500 [02:24<6:33:57, 47.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  1 idx: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 4/500 [02:50<5:24:22, 39.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  2 idx: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 5/500 [03:28<5:18:04, 38.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  2 idx: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|          | 6/500 [03:52<4:38:10, 33.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   1%|▏         | 7/500 [05:28<7:25:06, 54.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   2%|▏         | 8/500 [06:28<7:38:06, 55.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  3 idx: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating problems:   2%|▏         | 9/500 [07:17<7:19:12, 53.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  4 idx: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating problems:   2%|▏         | 10/500 [09:07<7:27:21, 54.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corrects :  4 idx: 9\n",
            "\n",
            "=== Results Summary ===\n",
            "Total problems: 7\n",
            "Correct answers: 2\n",
            "Accuracy: 28.57%\n",
            "\n",
            "=== Incorrect Problems ===\n",
            "Problem 0:\n",
            "Expected: \\left( 3, \\frac{\\pi}{2} \\right)\n",
            "Predicted: \\left(3,\\ \\dfrac{\\pi}{2}\\right)\n",
            "---\n",
            "Problem 1:\n",
            "Expected: p - q\n",
            "Predicted: None\n",
            "---\n",
            "Problem 6:\n",
            "Expected: 27\n",
            "Predicted: None\n",
            "---\n",
            "Problem 7:\n",
            "Expected: 90^\\circ\n",
            "Predicted: None\n",
            "---\n",
            "Problem 9:\n",
            "Expected: 4\n",
            "Predicted: None\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate_self_refiner()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DORqkZL-H81P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2be5c26c6c9e40d48b77b068ecae90e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f54b39cf8ba4a02ad9797a92a11d3fb",
              "IPY_MODEL_45892f31e1ad43f5b1b0519088a3ca89",
              "IPY_MODEL_a43084754d854912a156b02e293ee276"
            ],
            "layout": "IPY_MODEL_99b4919a604f4e3aaf87cbbd24709818"
          }
        },
        "7f54b39cf8ba4a02ad9797a92a11d3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_499d6c15224641948c1ca5bf18d548aa",
            "placeholder": "​",
            "style": "IPY_MODEL_47a2598c72304c899402da0d3da8473a",
            "value": "README.md: 100%"
          }
        },
        "45892f31e1ad43f5b1b0519088a3ca89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67260fcca1414dd58ea6a758737d39b4",
            "max": 412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0225ff670afb4a5d84c92756ab5a07ec",
            "value": 412
          }
        },
        "a43084754d854912a156b02e293ee276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_762fa159570e457f9777dce44ea358a4",
            "placeholder": "​",
            "style": "IPY_MODEL_2acbf422fd3c46d081a7ef94b6209aed",
            "value": " 412/412 [00:00&lt;00:00, 23.1kB/s]"
          }
        },
        "99b4919a604f4e3aaf87cbbd24709818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "499d6c15224641948c1ca5bf18d548aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47a2598c72304c899402da0d3da8473a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67260fcca1414dd58ea6a758737d39b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0225ff670afb4a5d84c92756ab5a07ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "762fa159570e457f9777dce44ea358a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2acbf422fd3c46d081a7ef94b6209aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfea52494976476b91c97a7115507e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a09c29455e184335863a17904a06a828",
              "IPY_MODEL_db6026c8677a453aafb5c81fe863691e",
              "IPY_MODEL_9bbe64615e324501b5a884f9a6482ef2"
            ],
            "layout": "IPY_MODEL_fba93e38eb3d45aea1db7f1b37a4d22f"
          }
        },
        "a09c29455e184335863a17904a06a828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8235316fdbf442db9a004f162074a37d",
            "placeholder": "​",
            "style": "IPY_MODEL_c46f4f449a8c4428aecf624fc8459d22",
            "value": "test.jsonl: 100%"
          }
        },
        "db6026c8677a453aafb5c81fe863691e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce1ce8fe0dc34121b5d47979f717d855",
            "max": 446564,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27b52da18f7a44ee940d71fc40ec5d82",
            "value": 446564
          }
        },
        "9bbe64615e324501b5a884f9a6482ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef087039c4fb4c568ab883772096047d",
            "placeholder": "​",
            "style": "IPY_MODEL_0e2654e61fd1430da765f2e7fcf6d203",
            "value": " 447k/447k [00:00&lt;00:00, 1.01MB/s]"
          }
        },
        "fba93e38eb3d45aea1db7f1b37a4d22f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8235316fdbf442db9a004f162074a37d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c46f4f449a8c4428aecf624fc8459d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce1ce8fe0dc34121b5d47979f717d855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27b52da18f7a44ee940d71fc40ec5d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef087039c4fb4c568ab883772096047d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e2654e61fd1430da765f2e7fcf6d203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fb5824dfc7540b0b7ab42b4c9c70382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a8846f857c34419a6ffcd85fbb9c367",
              "IPY_MODEL_7e04b7d32fc643abb972dc051e4d6d4d",
              "IPY_MODEL_bf4219daa33e4ec1a4dd57aa28954f80"
            ],
            "layout": "IPY_MODEL_ba7598ddf71646dcb1045dd3a62a48a9"
          }
        },
        "6a8846f857c34419a6ffcd85fbb9c367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a457886d44b43649ddde9d3ece462eb",
            "placeholder": "​",
            "style": "IPY_MODEL_bd4daa7d9f2e414aac94f1904f58f8ff",
            "value": "Generating test split: 100%"
          }
        },
        "7e04b7d32fc643abb972dc051e4d6d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97d7df36850e4233a1681000dc925c9a",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a2b0bfed3c24e918a4f45fa5b2a94ae",
            "value": 500
          }
        },
        "bf4219daa33e4ec1a4dd57aa28954f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f5dddd30e1d4bbc9107d1ab27c5fb82",
            "placeholder": "​",
            "style": "IPY_MODEL_a2763f34d0fc47349c17ef287d53e7f6",
            "value": " 500/500 [00:00&lt;00:00, 4679.14 examples/s]"
          }
        },
        "ba7598ddf71646dcb1045dd3a62a48a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a457886d44b43649ddde9d3ece462eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd4daa7d9f2e414aac94f1904f58f8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97d7df36850e4233a1681000dc925c9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a2b0bfed3c24e918a4f45fa5b2a94ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f5dddd30e1d4bbc9107d1ab27c5fb82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2763f34d0fc47349c17ef287d53e7f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5406c2cd2154172aa2797e5c9987cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0837f33ab53463a959729de57222015",
              "IPY_MODEL_1b7c92d1913b4241b6b10bdd8738c354",
              "IPY_MODEL_286de0a2ec774e82bb537ab989c96c95"
            ],
            "layout": "IPY_MODEL_cd96e94b84df4abd86dc2c889cfa3c78"
          }
        },
        "e0837f33ab53463a959729de57222015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_844e222daba1481396363d9d214a5266",
            "placeholder": "​",
            "style": "IPY_MODEL_e9879f0cf1174633a88b6cedda10f873",
            "value": "README.md: 100%"
          }
        },
        "1b7c92d1913b4241b6b10bdd8738c354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6af11a29c86c401a802a068152d48079",
            "max": 412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6abea01cd255435b990ca17507fb4c3e",
            "value": 412
          }
        },
        "286de0a2ec774e82bb537ab989c96c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ad413a211f741b490858f9afaf182ad",
            "placeholder": "​",
            "style": "IPY_MODEL_d597f98dd5b24e069fbd7b79e6272183",
            "value": " 412/412 [00:00&lt;00:00, 39.9kB/s]"
          }
        },
        "cd96e94b84df4abd86dc2c889cfa3c78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "844e222daba1481396363d9d214a5266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9879f0cf1174633a88b6cedda10f873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6af11a29c86c401a802a068152d48079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abea01cd255435b990ca17507fb4c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ad413a211f741b490858f9afaf182ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d597f98dd5b24e069fbd7b79e6272183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "286f1ea17e2f429d95476fac9fe3af79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0189bf64f5f84988af5428c095757ff4",
              "IPY_MODEL_1bfb0adeff664795858c952af063814a",
              "IPY_MODEL_b96f17f909fa4532a232deddc8cc948c"
            ],
            "layout": "IPY_MODEL_983c545b4aac462797b2f3975ce98f6b"
          }
        },
        "0189bf64f5f84988af5428c095757ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40544865d289438c9dce61fbc2c4d86c",
            "placeholder": "​",
            "style": "IPY_MODEL_c8fc4a20d51e4020aaf68c2910a98191",
            "value": "test.jsonl: 100%"
          }
        },
        "1bfb0adeff664795858c952af063814a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ddba8b63d644abb83f93ea03904eb9d",
            "max": 446564,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e852b563c9a94486bacabcabd6e218a4",
            "value": 446564
          }
        },
        "b96f17f909fa4532a232deddc8cc948c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_267a9fb61f6f497a9bfda725275fd043",
            "placeholder": "​",
            "style": "IPY_MODEL_f8d53d20a00c4e45a183832604b8edef",
            "value": " 447k/447k [00:00&lt;00:00, 5.99MB/s]"
          }
        },
        "983c545b4aac462797b2f3975ce98f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40544865d289438c9dce61fbc2c4d86c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8fc4a20d51e4020aaf68c2910a98191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ddba8b63d644abb83f93ea03904eb9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e852b563c9a94486bacabcabd6e218a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "267a9fb61f6f497a9bfda725275fd043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8d53d20a00c4e45a183832604b8edef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecb90d4131344f1a8e04ccaf5434f2ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_567735bc40a44c31945ac337a9928dfc",
              "IPY_MODEL_30eda6c38d2c406e9de0a0dd91e9ba3f",
              "IPY_MODEL_28cec9977ce2448790c20c2102050bb6"
            ],
            "layout": "IPY_MODEL_d46795c136b947dfb875547c76143463"
          }
        },
        "567735bc40a44c31945ac337a9928dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abf1600790b94f9c9226a663fae23f66",
            "placeholder": "​",
            "style": "IPY_MODEL_da1bc1bdecd340bd883dec01ca9ddef2",
            "value": "Generating test split: 100%"
          }
        },
        "30eda6c38d2c406e9de0a0dd91e9ba3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f671ce8f1af4065a9a2f71b8e1011c6",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47f54fee017745f493489af0d1fd7700",
            "value": 500
          }
        },
        "28cec9977ce2448790c20c2102050bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38305d58123a49628c7ac75aec288094",
            "placeholder": "​",
            "style": "IPY_MODEL_af290567c48c4a80be4e759d2fe8cf71",
            "value": " 500/500 [00:00&lt;00:00, 7361.19 examples/s]"
          }
        },
        "d46795c136b947dfb875547c76143463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abf1600790b94f9c9226a663fae23f66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da1bc1bdecd340bd883dec01ca9ddef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f671ce8f1af4065a9a2f71b8e1011c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47f54fee017745f493489af0d1fd7700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38305d58123a49628c7ac75aec288094": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af290567c48c4a80be4e759d2fe8cf71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b02d2b7fca045558485b128ea54f947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ab44b8b89084e64afa54e795bfd4ead",
              "IPY_MODEL_93075539ee1545e3830d1af7ae64b641",
              "IPY_MODEL_d94cf06e2d1f43e796992896fbd17933"
            ],
            "layout": "IPY_MODEL_3a8f7faa13074d8d862d6d727d509d9f"
          }
        },
        "7ab44b8b89084e64afa54e795bfd4ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_126f2dac817f48bb971d94b5be7a220d",
            "placeholder": "​",
            "style": "IPY_MODEL_c3377123649f43dc81f728d91824cffb",
            "value": "README.md: 100%"
          }
        },
        "93075539ee1545e3830d1af7ae64b641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fa6d16ed7114ff8b3fba682c8d67c7e",
            "max": 412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8e959d08263459c8fffb922c31a0e06",
            "value": 412
          }
        },
        "d94cf06e2d1f43e796992896fbd17933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9271ddc83a46439b8e5a6730d52052c0",
            "placeholder": "​",
            "style": "IPY_MODEL_e4bf36df1a824bd98aa59d100bb13578",
            "value": " 412/412 [00:00&lt;00:00, 29.5kB/s]"
          }
        },
        "3a8f7faa13074d8d862d6d727d509d9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "126f2dac817f48bb971d94b5be7a220d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3377123649f43dc81f728d91824cffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fa6d16ed7114ff8b3fba682c8d67c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8e959d08263459c8fffb922c31a0e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9271ddc83a46439b8e5a6730d52052c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4bf36df1a824bd98aa59d100bb13578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2189a88b16ac4a94b726445c2420feeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2fc0d1c048b4f6186680a429c7b6980",
              "IPY_MODEL_869f0ada92a24619882392c5b084f994",
              "IPY_MODEL_7e997598a48b4db6a5475f4d306456e9"
            ],
            "layout": "IPY_MODEL_cfb6c2a159c44058b2b087875dd952f3"
          }
        },
        "f2fc0d1c048b4f6186680a429c7b6980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61a60fb35bf3494db58990be024309f3",
            "placeholder": "​",
            "style": "IPY_MODEL_364049304ed340baa24843c270a4b7f2",
            "value": "test.jsonl: 100%"
          }
        },
        "869f0ada92a24619882392c5b084f994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1b632d9be7348f784df0d4b10ae9010",
            "max": 446564,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3058efaa40954732b0755a08e46eda9f",
            "value": 446564
          }
        },
        "7e997598a48b4db6a5475f4d306456e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239c519be75d4efb964899f01e1b4748",
            "placeholder": "​",
            "style": "IPY_MODEL_2d08afc27f914036a30a3c06d6ea7bf9",
            "value": " 447k/447k [00:00&lt;00:00, 5.92MB/s]"
          }
        },
        "cfb6c2a159c44058b2b087875dd952f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a60fb35bf3494db58990be024309f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "364049304ed340baa24843c270a4b7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1b632d9be7348f784df0d4b10ae9010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3058efaa40954732b0755a08e46eda9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "239c519be75d4efb964899f01e1b4748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d08afc27f914036a30a3c06d6ea7bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d67d1731d2f34063aa2cb51c39089f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccbbcb77baa4460ea0fad800c4e915f2",
              "IPY_MODEL_e9fa243f73c84821a8dcc6146f8b1815",
              "IPY_MODEL_e6aaca4042e0427ebadc413cc10637fc"
            ],
            "layout": "IPY_MODEL_6cf63adeea144de29bb332709efb224c"
          }
        },
        "ccbbcb77baa4460ea0fad800c4e915f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ae5d91cd9f748beb49ac6256f36f9a5",
            "placeholder": "​",
            "style": "IPY_MODEL_8b7da10d996748729f5171322a9f391b",
            "value": "Generating test split: 100%"
          }
        },
        "e9fa243f73c84821a8dcc6146f8b1815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a7293bc156f479c91b17b6c0c23f167",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47a5d84e1a18400881d14f658f596f46",
            "value": 500
          }
        },
        "e6aaca4042e0427ebadc413cc10637fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5684f8b0f7c4b8bbccc2fa9cecf4ad0",
            "placeholder": "​",
            "style": "IPY_MODEL_13ed151f6a0b45a28e942649934a9d44",
            "value": " 500/500 [00:00&lt;00:00, 8105.37 examples/s]"
          }
        },
        "6cf63adeea144de29bb332709efb224c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae5d91cd9f748beb49ac6256f36f9a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b7da10d996748729f5171322a9f391b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a7293bc156f479c91b17b6c0c23f167": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47a5d84e1a18400881d14f658f596f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5684f8b0f7c4b8bbccc2fa9cecf4ad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ed151f6a0b45a28e942649934a9d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}